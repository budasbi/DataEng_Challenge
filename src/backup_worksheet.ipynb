{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DATA_FILES, DATABASE_HOST, DATABASE_NAME, DATABASE_PASS, DATABASE_USER, BUCKET, BACKUPS_FILES\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import fastavro\n",
    "from load_data_to_db.copy_to_db import copy_expert\n",
    "\n",
    "\n",
    "\n",
    "def create_table_backup(table_name):\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "    dbname=DATABASE_NAME,\n",
    "    user=DATABASE_USER,\n",
    "    password=DATABASE_PASS,\n",
    "    host=DATABASE_HOST,\n",
    "    port='5432'\n",
    "    )\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"select * from {table_name}\")\n",
    "    records = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    if table_name == 'jobs':\n",
    "        schema = {\n",
    "                \"type\": \"record\",\n",
    "                \"name\": \"jobs\",\n",
    "                \"fields\": [\n",
    "                    {\"name\": \"id\", \"type\": \"int\"},\n",
    "                    {\"name\": \"job\", \"type\": \"string\"},\n",
    "                ]\n",
    "            }\n",
    "        avro_records = [{\"id\": r[0], \"job\": r[1]} for r in records]\n",
    "    elif table_name == 'departments':\n",
    "        schema = {\n",
    "                \"type\": \"record\",\n",
    "                \"name\": \"departments\",\n",
    "                \"fields\": [\n",
    "                    {\"name\": \"id\", \"type\": \"int\"},\n",
    "                    {\"name\": \"department\", \"type\": \"string\"},\n",
    "                ]\n",
    "            }\n",
    "        avro_records = [{\"id\": r[0], \"department\": r[1]} for r in records]\n",
    "    elif table_name == 'hired_employees':\n",
    "        schema = {\n",
    "                \"type\": \"record\",\n",
    "                \"name\": \"jobs\",\n",
    "                \"fields\": [\n",
    "                    {\"name\": \"id\", \"type\": \"int\"},\n",
    "                    {\"name\": \"name\", \"type\": \"string\"},\n",
    "                    {\"name\": \"datetime\", \"type\": \"string\"},\n",
    "                    {\"name\": \"department_id\", \"type\": \"int\"},\n",
    "                    {\"name\": \"job_id\", \"type\": \"int\"}\n",
    "                ]\n",
    "            }\n",
    "        avro_records = [{\"id\": r[0], \"name\": r[1], \"datetime\": r[2], \"department_id\": r[3], \"job_id\": r[4]} for r in records]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    backup_path_file = os.path.join(BACKUPS_FILES,f'{table_name}.avro')\n",
    "    \n",
    "    #Write file\n",
    "    with open(backup_path_file, 'wb') as out:\n",
    "        fastavro.writer(out, schema, avro_records)\n",
    "        \n",
    "    s3Client = boto3.client('s3')    \n",
    "    #upload_file\n",
    "    with open(backup_path_file, 'rb') as fileObj:\n",
    "        response = s3Client.upload_fileobj(fileObj, BUCKET, f'backups/{table_name}.avro')\n",
    "        print(response)\n",
    "\n",
    "\n",
    "def get_backups(table_name):\n",
    "    \"\"\"Downloads jobs, departments, hired_employees backups files from s3\n",
    "    \"\"\"\n",
    "    os.environ['AWS_DEFAULT_REGION']='us-east-1'\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.download_file('data-challenge-bucket-oscar',f'backups/{table_name}.avro',os.path.join(BACKUPS_FILES,f'{table_name}.avro'))\n",
    "    \n",
    "def restore_backup(table_name):\n",
    "    get_backups()\n",
    "    backup_path_file = os.path.join(BACKUPS_FILES,f'{table_name}.avro')\n",
    "    with open(backup_path_file, 'rb') as file:\n",
    "        reader = fastavro.reader(file)\n",
    "        records = [record for record in reader]\n",
    "    table_data = pd.DataFrame.from_records(records)\n",
    "    copy_expert(df=table_data, table=table_name)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def backup_all_tables():\n",
    "    table_list = ['jobs', 'departments','hired_employees']\n",
    "    for table in table_list:\n",
    "        create_table_backup(table)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_all_tables():\n",
    "    table_list = ['jobs', 'departments','hired_employees']\n",
    "    for table in table_list:\n",
    "        restore_backup(table)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
